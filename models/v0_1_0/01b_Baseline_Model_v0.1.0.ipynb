{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1955cceb",
   "metadata": {},
   "source": [
    "# üèÅ 01b ‚Äì Baseline Model: Grid Position Rule with Versioned Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093a281",
   "metadata": {},
   "source": [
    "## üéØ Objective\n",
    "Establish a rule-based baseline for podium prediction:\n",
    "> Predict podium = `True` if grid position ‚â§ 3; otherwise, `False`.\n",
    "\n",
    "Automatically saves results to versioned folders for `v0.1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Setup paths for versioned output\n",
    "import os\n",
    "version = 'v0.1.0'\n",
    "os.makedirs(f'models/{version}', exist_ok=True)\n",
    "os.makedirs(f'reports/{version}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Create mock data for a single race\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "drivers = [f'Driver_{i+1}' for i in range(20)]\n",
    "grid_positions = list(range(1, 21))\n",
    "true_podium = [1 if i < 3 else 0 for i in np.random.permutation(20)]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Driver': drivers,\n",
    "    'GridPosition': grid_positions,\n",
    "    'TruePodium': true_podium\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üö¶ Apply baseline rule\n",
    "df['BaselinePrediction'] = (df['GridPosition'] <= 3).astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3237589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Evaluate performance\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score\n",
    "import json\n",
    "\n",
    "precision = precision_score(df['TruePodium'], df['BaselinePrediction'])\n",
    "f1 = f1_score(df['TruePodium'], df['BaselinePrediction'])\n",
    "accuracy = accuracy_score(df['TruePodium'], df['BaselinePrediction'])\n",
    "\n",
    "metrics = {\n",
    "    'precision': round(precision, 4),\n",
    "    'f1_score': round(f1, 4),\n",
    "    'accuracy': round(accuracy, 4)\n",
    "}\n",
    "print(metrics)\n",
    "\n",
    "# Save metrics to reports\n",
    "with open(f'reports/{version}/baseline_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537c3c9",
   "metadata": {},
   "source": [
    "## ‚úÖ Output Saved\n",
    "- `reports/v0.1.0/baseline_metrics.json`\n",
    "- No model object saved (rule-based, no training needed)\n",
    "\n",
    "This establishes a performance baseline for comparison with future ML models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
